# ü§ñ Ollama Chatbot com Flask

Este projeto √© um aplicativo de chat simples e moderno que usa uma interface web para se comunicar com um servidor **Ollama** local. O backend em Python com Flask atua como uma ponte, enviando suas perguntas para o Ollama e recebendo as respostas de modelos de linguagem grandes.

## ‚ú® Funcionalidades

- **Interface de Usu√°rio Responsiva:** Um front-end limpo e responsivo, estilizado com **Tailwind CSS**.
- **Backend Robusto:** Um servidor **Flask** que gerencia a comunica√ß√£o entre o front-end e a API do Ollama.
- **Integra√ß√£o com Ollama:** Conecta-se a um servidor Ollama local para gerar respostas usando modelos como o Mistral.
- **Comunica√ß√£o Ass√≠ncrona:** Usa a API `fetch` para interagir com o backend sem recarregar a p√°gina.

## üöÄ Tecnologias Utilizadas

- **Frontend:** HTML, JavaScript, Tailwind CSS
- **Backend:** Python, Flask, Flask-CORS, `requests`
- **AI:** Ollama

## üì¶ Pr√©-requisitos

Para executar este projeto, voc√™ precisar√° ter o seguinte instalado:

- **Python 3.8+** e `pip`.
- **Ollama:** Instale e execute o Ollama em sua m√°quina. O modelo **Mistral** √© o padr√£o para este projeto. Voc√™ pode baix√°-lo com o comando `ollama run mistral`.

## ‚öôÔ∏è Como Come√ßar

Siga estes passos para configurar e executar a aplica√ß√£o:

1.  **Salve os arquivos:** Certifique-se de que o c√≥digo HTML (`index.html`) e o script Python (`ollama_api.py`) estejam na mesma pasta.

2.  **Crie e ative um ambiente virtual** (recomendado):
    ```bash
    python3 -m venv venv
    # No macOS/Linux
    source venv/bin/activate
    # No Windows
    .\venv\Scripts\activate
    ```

3.  **Instale as depend√™ncias do Python:**
    Com o ambiente virtual ativado, execute:
    ```bash
    pip install Flask flask-cors requests
    ```

4.  **Inicie o servidor Flask:**
    Abra seu terminal na pasta do projeto e execute:
    ```bash
    python3 ollama_api.py
    ```
    O servidor da API estar√° rodando em `http://127.0.0.1:5000`.

5.  **Inicie o Ollama:**
    Em um terminal separado, certifique-se de que o servidor Ollama est√° em execu√ß√£o. Se ainda n√£o tiver o modelo `mistral`, baixe-o e inicie-o com:
    ```bash
    ollama run mistral
    ```

6.  **Abra o Frontend:**
    Abra o arquivo `index.html` diretamente em seu navegador. A interface se conectar√° automaticamente ao servidor Flask em execu√ß√£o.
